{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saharkhsrvi/hugging-face-demo1/blob/main/Copy_of_Dandy_CV_Practical_Exercise_(2025_05_12_Sahar_Khosravi).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# __CV Practical Exercise__\n",
        "\n",
        "In this exercise, you are given a script that trains a simple image classification model on a generated dataset of differently colored shapes.\n",
        "\n",
        "Currently, this script does not achieve good accuracy on a validation dataset. Your task is to find ways to improve the accuracy of this model. Given the limited time, you might want to do some planning and prioritization before jumping into this exercise. Note that there are no deliberate coding bugs that were introduced; rather, the code was written by an inexperienced researcher.\n",
        "\n",
        "You may use online resources and references as needed."
      ],
      "metadata": {
        "id": "6yaEyx0QTLlI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import definitions"
      ],
      "metadata": {
        "id": "sNDKkj3IVmnD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hUlCgsYLjXN8"
      },
      "outputs": [],
      "source": [
        "from dataclasses import dataclass\n",
        "import collections\n",
        "import json\n",
        "import multiprocessing\n",
        "import os\n",
        "import traceback\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "from torch.autograd import Variable\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data processing routines"
      ],
      "metadata": {
        "id": "YxoJDaa3VYqs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class Datapoint:\n",
        "    \"\"\"Represents one datapoint.\n",
        "    This structure is flexible enough to support both detection and segmentation.\"\"\"\n",
        "    image_id: int\n",
        "    image: np.ndarray  # (H, W, 3) dtype uint8 representing image\n",
        "    boxes: np.ndarray  # (N, 5) dtype np.int32 representing list of boxes (xmin, ymin, width, height, class)\n",
        "    mask: np.ndarray  # (H, W) dtype np.int32 representing class mask\n",
        "\n",
        "def read_dataset_categories(dataset_dir, split_name):\n",
        "    with open(f'{dataset_dir}/annotations/instances_{split_name}.json', 'r') as fi:\n",
        "        obj = json.load(fi)\n",
        "    return obj['categories']\n",
        "\n",
        "def read_dataset(dataset_dir, split_name):\n",
        "    with open(f'{dataset_dir}/annotations/instances_{split_name}.json', 'r') as fi:\n",
        "        obj = json.load(fi)\n",
        "\n",
        "    # Load annotations into an index\n",
        "    index = collections.defaultdict(list)  # map of image id to annotation\n",
        "    for annotation in obj['annotations']:\n",
        "        index[annotation['image_id']].append(annotation)\n",
        "\n",
        "    # Iterate through images\n",
        "    images_info = obj['images']\n",
        "    for i, image_info in enumerate(images_info):\n",
        "        filename = image_info['file_name']\n",
        "        annotations = index[image_info['id']]\n",
        "\n",
        "        with Image.open(os.path.join(dataset_dir, split_name, filename)) as f:\n",
        "            img = np.asarray(f)\n",
        "\n",
        "        mask = np.zeros(img.shape[:2], dtype=np.int32)\n",
        "        boxes = []\n",
        "        for annotation in annotations:\n",
        "            xmin, ymin, width, height = map(int, annotation['bbox'])\n",
        "            class_id = annotation['category_id']\n",
        "            boxes.append([xmin, ymin, width, height, class_id])\n",
        "            mask[ymin: ymin + height, xmin: xmin + width] = class_id\n",
        "\n",
        "        yield Datapoint(image_info['id'], img, boxes, mask)\n"
      ],
      "metadata": {
        "id": "mES5JKsIDFgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_transforms = transforms.Compose([transforms.RandomHorizontalFlip(),\n",
        "                                      transforms.ColorJitter(brightness=0.1, contrast =0.1),\n",
        "                                      transforms.ToTensor(),\n",
        "                                      transforms.Normalize(mean=0.2, std=0.2),\n",
        "                                      ])\n",
        "\n",
        "val_transforms = transforms.Compose([transforms.ToTensor(),transforms.Normalize(mean=0.2, std=0.2)])"
      ],
      "metadata": {
        "id": "I_VlMtSWWYsr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset definition"
      ],
      "metadata": {
        "id": "U71TMWTFVuTh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, dataset_dir, split_name, transforms=None):\n",
        "        self.datapoints = list(read_dataset(dataset_dir, split_name))\n",
        "        self.transform = transforms\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        datapoint = self.datapoints[idx]\n",
        "        img = datapoint.image.transpose(2, 0, 1).copy()  # (C, H, W)\n",
        "        target = datapoint.boxes[0][-1] - 1\n",
        "        img = torch.as_tensor(img, dtype=torch.float32)\n",
        "        target = torch.tensor(target, dtype=torch.int64)\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.datapoints)\n"
      ],
      "metadata": {
        "id": "WCiPID6yDP9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Network architecture definition"
      ],
      "metadata": {
        "id": "gJxfRPQrWFaq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Network(nn.Module):\n",
        "    def __init__(self, height, width, num_classes):\n",
        "        super(Network, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=12, kernel_size=5, stride=1, padding=1)\n",
        "        self.bn1 = nn.BatchNorm2d(12)\n",
        "        self.conv2 = nn.Conv2d(in_channels=12, out_channels=12, kernel_size=5, stride=1, padding=1)\n",
        "        self.bn2 = nn.BatchNorm2d(12)\n",
        "        self.pool = nn.MaxPool2d(2,2)\n",
        "        self.conv4 = nn.Conv2d(in_channels=12, out_channels=24, kernel_size=5, stride=1, padding=1)\n",
        "        self.bn4 = nn.BatchNorm2d(24)\n",
        "        self.conv5 = nn.Conv2d(in_channels=24, out_channels=24, kernel_size=5, stride=1, padding=1)\n",
        "        self.bn5 = nn.BatchNorm2d(24)\n",
        "        h = (height - 4) // 2 - 4\n",
        "        w = (width - 4) // 2 - 4\n",
        "        self.fc_input = 24 * h * w\n",
        "        self.fc1 = nn.Linear(self.fc_input, num_classes)\n",
        "\n",
        "    def forward(self, input):\n",
        "        output = F.relu(self.bn1(self.conv1(input)))\n",
        "        output = F.relu(self.bn2(self.conv2(output)))\n",
        "        output = self.pool(output)\n",
        "        output = F.relu(self.bn4(self.conv4(output)))\n",
        "        output = F.relu(self.bn5(self.conv5(output)))\n",
        "        output = output.view(-1, self.fc_input)\n",
        "        output = self.fc1(output)\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "HYPfczCJV4YH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f9do-Bm-VK_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "mrUgtUCBVjYh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training loop definition"
      ],
      "metadata": {
        "id": "SqxZ-fGaWLj-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(dataset_dir, transforms):\n",
        "    dataset_train = Dataset(dataset_dir, 'train')\n",
        "    dataset_val = Dataset(dataset_dir, 'val')\n",
        "    categories = read_dataset_categories(dataset_dir, 'train')\n",
        "    num_classes = len(categories)\n",
        "    height = 100\n",
        "    width = 120\n",
        "\n",
        "    dataloader_train = torch.utils.data.DataLoader(\n",
        "        dataset_train, batch_size=10, shuffle=True, num_workers=2)\n",
        "    dataloader_val = torch.utils.data.DataLoader(\n",
        "        dataset_val, batch_size=1, shuffle=False, num_workers=2)\n",
        "\n",
        "    model = Network(height, width, num_classes)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    optimizer = Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
        "\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    num_epochs = 100\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for i, (images, labels) in enumerate(dataloader_train, 0):\n",
        "            images = Variable(images.to(device))\n",
        "            labels = Variable(labels.to(device))\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "        train_loss /= len(dataloader_train)\n",
        "        val_acc = evaluate(model, device, dataloader_val)\n",
        "        print(f'epoch {epoch} loss: {train_loss:.4f}, val acc: {val_acc}')\n"
      ],
      "metadata": {
        "id": "8T15IuhDV6Pe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation function definition"
      ],
      "metadata": {
        "id": "0dDA38r5WOCw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, device, dataloader):\n",
        "    model.eval()\n",
        "    accuracy = 0.0\n",
        "    total = 0.0\n",
        "    with torch.no_grad():\n",
        "        for data in dataloader:\n",
        "            images, labels = data\n",
        "            images = images.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            predicted = predicted.cpu()\n",
        "            total += labels.size(0)\n",
        "            accuracy += (predicted == labels).sum().item()\n",
        "    return accuracy / total\n"
      ],
      "metadata": {
        "id": "T69j_AQ6V7go"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let's train the model!\n"
      ],
      "metadata": {
        "id": "9Y6GAkFwV9M2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_dir = 'datasets/image_classification'"
      ],
      "metadata": {
        "id": "AoSnvUD9QF14"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datapoints = list(read_dataset('datasets/image_classification', 'train'))\n",
        "\n",
        "labels = [datapoint.boxes[0][-1] for datapoint in datapoints]\n",
        "counts = Counter(labels)\n",
        "print(\"Counts of labels: \\n\", counts)\n",
        "\n",
        "plt.bar(counts.keys(), counts.values())\n",
        "plt.xlabel(\"Class Label\")\n",
        "plt.ylabel(\"Counts\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 484
        },
        "id": "TIZnLuPOYjz_",
        "outputId": "b917f6cd-f2cf-4955-aa51-0e3efb9e97f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counts of labels: \n",
            " Counter({8: 76, 9: 76, 10: 76, 4: 40, 5: 40, 6: 40, 7: 40, 1: 4, 2: 4, 3: 4})\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjIAAAGwCAYAAACzXI8XAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAoFElEQVR4nO3df1SUdd7/8dcgMJDCKKwOkqBoGmj+Sk3J7n4YK2vmnUe2co91u2W22+IPYLc21vyRWVht6W2hpreL9+7GcXM3LWvTVTTdNjDFaLUfmGlBIdiuwqgbA8n1/WNP871JURiRaz70fJxzneNcn2su3sw56vNccw04LMuyBAAAYKAguwcAAADwFyEDAACMRcgAAABjETIAAMBYhAwAADAWIQMAAIxFyAAAAGMF2z3ApdbQ0KCKigpFRETI4XDYPQ4AAGgGy7J08uRJxcbGKiio6esu7T5kKioqFBcXZ/cYAADAD+Xl5erRo0eT6+0+ZCIiIiT9+4WIjIy0eRoAANAcHo9HcXFxvv/Hm9LuQ+abt5MiIyMJGQAADHOh20K42RcAABiLkAEAAMYiZAAAgLEIGQAAYCxCBgAAGIuQAQAAxiJkAACAsQgZAABgLEIGAAAYi5ABAADGImQAAICxCBkAAGAsQgYAABiLkAEAAMYiZAAAgLGC7R4AAGC2Xg+/bvcIjXy6eHyzjmPu1tHcuS8VrsgAAABjETIAAMBYhAwAADAWIQMAAIxFyAAAAGMRMgAAwFiEDAAAMBYhAwAAjEXIAAAAYxEyAADAWIQMAAAwFiEDAACMRcgAAABjETIAAMBYhAwAADAWIQMAAIxFyAAAAGMRMgAAwFiEDAAAMBYhAwAAjEXIAAAAYxEyAADAWIQMAAAwlq0h06tXLzkcjrO29PR0SVJtba3S09MVHR2tTp06KS0tTVVVVXaODAAAAoitIbNnzx4dPXrUt23dulWSdPvtt0uSMjMztWnTJq1fv147d+5URUWFJk2aZOfIAAAggATb+cW7du3a6PHixYvVp08f3XDDDaqpqdGaNWuUn5+vMWPGSJLy8vKUlJSkoqIijRo1yo6RAQBAAAmYe2Tq6ur0+9//Xvfee68cDoeKi4tVX1+vlJQU3zGJiYmKj49XYWFhk+fxer3yeDyNNgAA0D4FTMhs3LhR1dXV+vGPfyxJqqysVGhoqDp37tzoOLfbrcrKyibPk5OTI5fL5dvi4uIu4dQAAMBOARMya9as0bhx4xQbG3tR58nOzlZNTY1vKy8vb6UJAQBAoLH1HplvfPbZZ9q2bZtefvll376YmBjV1dWpurq60VWZqqoqxcTENHkup9Mpp9N5KccFAAABIiCuyOTl5albt24aP368b9+wYcMUEhKigoIC377S0lKVlZUpOTnZjjEBAECAsf2KTENDg/Ly8jR16lQFB///cVwul6ZNm6asrCxFRUUpMjJSM2fOVHJyMp9YAgAAkgIgZLZt26aysjLde++9Z60tWbJEQUFBSktLk9frVWpqqpYvX27DlAAAIBDZHjJjx46VZVnnXAsLC1Nubq5yc3PbeCoAAGCCgLhHBgAAwB+EDAAAMBYhAwAAjEXIAAAAYxEyAADAWIQMAAAwFiEDAACMRcgAAABjETIAAMBYhAwAADAWIQMAAIxFyAAAAGMRMgAAwFiEDAAAMBYhAwAAjEXIAAAAYxEyAADAWIQMAAAwFiEDAACMRcgAAABjETIAAMBYhAwAADAWIQMAAIxFyAAAAGMRMgAAwFiEDAAAMBYhAwAAjEXIAAAAYxEyAADAWIQMAAAwFiEDAACMRcgAAABjETIAAMBYhAwAADAWIQMAAIxle8h88cUXuuuuuxQdHa3w8HANHDhQe/fu9a1blqV58+ape/fuCg8PV0pKij7++GMbJwYAAIHC1pA5ceKERo8erZCQEL3xxhv64IMP9Mwzz6hLly6+Y5566iktW7ZMK1eu1O7du9WxY0elpqaqtrbWxskBAEAgCLbziz/55JOKi4tTXl6eb19CQoLvz5ZlaenSpXrkkUd02223SZJ++9vfyu12a+PGjZo8eXKbzwwAAAKHrVdkXn31VQ0fPly33367unXrpqFDh2r16tW+9SNHjqiyslIpKSm+fS6XSyNHjlRhYeE5z+n1euXxeBptAACgfbI1ZA4fPqwVK1aob9++2rJlix544AHNmjVL//u//ytJqqyslCS53e5Gz3O73b61b8vJyZHL5fJtcXFxl/abAAAAtrE1ZBoaGnT11VfriSee0NChQ3X//fdr+vTpWrlypd/nzM7OVk1NjW8rLy9vxYkBAEAgsTVkunfvrv79+zfal5SUpLKyMklSTEyMJKmqqqrRMVVVVb61b3M6nYqMjGy0AQCA9snWkBk9erRKS0sb7Tt48KB69uwp6d83/sbExKigoMC37vF4tHv3biUnJ7fprAAAIPDY+qmlzMxMXXvttXriiSd0xx136J133tGqVau0atUqSZLD4VBGRoYWLVqkvn37KiEhQXPnzlVsbKwmTpxo5+gAACAA2BoyI0aM0IYNG5Sdna2FCxcqISFBS5cu1ZQpU3zHPPTQQzp9+rTuv/9+VVdX67rrrtPmzZsVFhZm4+QAACAQ2BoyknTrrbfq1ltvbXLd4XBo4cKFWrhwYRtOBQAATGD7rygAAADwFyEDAACMRcgAAABjETIAAMBYhAwAADAWIQMAAIxFyAAAAGMRMgAAwFiEDAAAMBYhAwAAjEXIAAAAYxEyAADAWIQMAAAwFiEDAACMRcgAAABjETIAAMBYhAwAADAWIQMAAIxFyAAAAGMRMgAAwFiEDAAAMBYhAwAAjEXIAAAAYxEyAADAWIQMAAAwFiEDAACMRcgAAABjETIAAMBYhAwAADAWIQMAAIxFyAAAAGMRMgAAwFiEDAAAMBYhAwAAjEXIAAAAYxEyAADAWLaGzIIFC+RwOBptiYmJvvXa2lqlp6crOjpanTp1UlpamqqqqmycGAAABBLbr8gMGDBAR48e9W1vvfWWby0zM1ObNm3S+vXrtXPnTlVUVGjSpEk2TgsAAAJJsO0DBAcrJibmrP01NTVas2aN8vPzNWbMGElSXl6ekpKSVFRUpFGjRp3zfF6vV16v1/fY4/FcmsEBAIDtbL8i8/HHHys2Nla9e/fWlClTVFZWJkkqLi5WfX29UlJSfMcmJiYqPj5ehYWFTZ4vJydHLpfLt8XFxV3y7wEAANjD1pAZOXKk1q5dq82bN2vFihU6cuSI/uM//kMnT55UZWWlQkND1blz50bPcbvdqqysbPKc2dnZqqmp8W3l5eWX+LsAAAB2sfWtpXHjxvn+PGjQII0cOVI9e/bUSy+9pPDwcL/O6XQ65XQ6W2tEAAAQwGx/a+n/6ty5s/r166dDhw4pJiZGdXV1qq6ubnRMVVXVOe+pAQAA3z0BFTKnTp3SJ598ou7du2vYsGEKCQlRQUGBb720tFRlZWVKTk62cUoAABAobH1r6Re/+IUmTJignj17qqKiQvPnz1eHDh30ox/9SC6XS9OmTVNWVpaioqIUGRmpmTNnKjk5uclPLAEAgO8WW0Pm888/149+9CP985//VNeuXXXdddepqKhIXbt2lSQtWbJEQUFBSktLk9frVWpqqpYvX27nyAAAIIDYGjLr1q0773pYWJhyc3OVm5vbRhMBAACTBNQ9MgAAAC1ByAAAAGMRMgAAwFiEDAAAMBYhAwAAjEXIAAAAYxEyAADAWIQMAAAwFiEDAACMRcgAAABjETIAAMBYhAwAADAWIQMAAIxFyAAAAGMRMgAAwFiEDAAAMBYhAwAAjEXIAAAAY/kVMvv27dP+/ft9j1955RVNnDhRv/rVr1RXV9dqwwEAAJyPXyHzk5/8RAcPHpQkHT58WJMnT9Zll12m9evX66GHHmrVAQEAAJriV8gcPHhQQ4YMkSStX79e119/vfLz87V27Vr96U9/as35AAAAmuRXyFiWpYaGBknStm3bdMstt0iS4uLi9I9//KP1pgMAADgPv0Jm+PDhWrRokX73u99p586dGj9+vCTpyJEjcrvdrTogAABAU/wKmSVLlmjfvn2aMWOG5syZoyuuuEKS9Mc//lHXXnttqw4IAADQlGB/njR48OBGn1r6xtNPP63gYL9OCQAA0GJ+XZHp3bu3/vnPf561v7a2Vv369bvooQAAAJrDr5D59NNPdebMmbP2e71eff755xc9FAAAQHO06H2gV1991ffnLVu2yOVy+R6fOXNGBQUFSkhIaL3pAAAAzqNFITNx4kRJksPh0NSpUxuthYSEqFevXnrmmWdabTgAAIDzaVHIfPOzYxISErRnzx5973vfuyRDAQAANIdfHzE6cuRIa88BAADQYn5/VrqgoEAFBQU6duyY70rNN37zm99c9GAAAAAX4lfIPProo1q4cKGGDx+u7t27y+FwtPZcAAAAF+RXyKxcuVJr167V3Xff3drzAAAANJtfP0emrq6OX0UAAABs51fI3HfffcrPz2/VQRYvXiyHw6GMjAzfvtraWqWnpys6OlqdOnVSWlqaqqqqWvXrAgAAc/n11lJtba1WrVqlbdu2adCgQQoJCWm0/uyzz7bofHv27NELL7ygQYMGNdqfmZmp119/XevXr5fL5dKMGTM0adIk/e1vf/NnbAAA0M74FTJ///vfNWTIEEnSgQMHGq219MbfU6dOacqUKVq9erUWLVrk219TU6M1a9YoPz9fY8aMkSTl5eUpKSlJRUVFGjVq1DnP5/V65fV6fY89Hk+L5gEAAObwK2R27NjRagOkp6dr/PjxSklJaRQyxcXFqq+vV0pKim9fYmKi4uPjVVhY2GTI5OTk6NFHH221+YCL1evh1+0e4SyfLh5/wWNMnVsKvNnb+9yAnfy6R6a1rFu3Tvv27VNOTs5Za5WVlQoNDVXnzp0b7Xe73aqsrGzynNnZ2aqpqfFt5eXlrT02AAAIEH5dkbnpppvO+xbS9u3bL3iO8vJyzZ49W1u3blVYWJg/Y5yT0+mU0+lstfMBAIDA5VfIfHN/zDfq6+tVUlKiAwcOnPXLJJtSXFysY8eO6eqrr/btO3PmjHbt2qXnn39eW7ZsUV1dnaqrqxtdlamqqlJMTIw/YwMAgHbGr5BZsmTJOfcvWLBAp06datY5br75Zu3fv7/RvnvuuUeJiYn65S9/qbi4OIWEhKigoEBpaWmSpNLSUpWVlSk5OdmfsQEAQDvj9+9aOpe77rpL11xzjX79619f8NiIiAhdddVVjfZ17NhR0dHRvv3Tpk1TVlaWoqKiFBkZqZkzZyo5ObnJG30BAMB3S6uGTGFhYave77JkyRIFBQUpLS1NXq9XqampWr58eaudHwAAmM2vkJk0aVKjx5Zl6ejRo9q7d6/mzp3r9zBvvvlmo8dhYWHKzc1Vbm6u3+cEAADtl18h43K5Gj0OCgrSlVdeqYULF2rs2LGtMhgAAMCF+BUyeXl5rT0HAABAi13UPTLFxcX68MMPJUkDBgzQ0KFDW2UoAACA5vArZI4dO6bJkyfrzTff9P2Ml+rqat10001at26dunbt2pozAgAAnJNfv6Jg5syZOnnypN5//30dP35cx48f14EDB+TxeDRr1qzWnhEAAOCc/Lois3nzZm3btk1JSUm+ff3791dubi43+wIAgDbj1xWZhoYGhYSEnLU/JCREDQ0NFz0UAABAc/gVMmPGjNHs2bNVUVHh2/fFF18oMzNTN998c6sNBwAAcD5+hczzzz8vj8ejXr16qU+fPurTp48SEhLk8Xj03HPPtfaMAAAA5+TXPTJxcXHat2+ftm3bpo8++kiSlJSUpJSUlFYdDgAA4HxadEVm+/bt6t+/vzwejxwOh77//e9r5syZmjlzpkaMGKEBAwbor3/966WaFQAAoJEWhczSpUs1ffp0RUZGnrXmcrn0k5/8RM8++2yrDQcAAHA+LQqZ9957Tz/4wQ+aXB87dqyKi4sveigAAIDmaFHIVFVVnfNj198IDg7Wl19+edFDAQAANEeLQubyyy/XgQMHmlz/+9//ru7du1/0UAAAAM3RopC55ZZbNHfuXNXW1p619tVXX2n+/Pm69dZbW204AACA82nRx68feeQRvfzyy+rXr59mzJihK6+8UpL00UcfKTc3V2fOnNGcOXMuyaAAAADf1qKQcbvdevvtt/XAAw8oOztblmVJkhwOh1JTU5Wbmyu3231JBgUAAPi2Fv9AvJ49e+rPf/6zTpw4oUOHDsmyLPXt21ddunS5FPMBAAA0ya+f7CtJXbp00YgRI1pzFgAAgBbx63ctAQAABAJCBgAAGIuQAQAAxiJkAACAsQgZAABgLEIGAAAYi5ABAADGImQAAICxCBkAAGAsQgYAABiLkAEAAMYiZAAAgLEIGQAAYCxCBgAAGIuQAQAAxrI1ZFasWKFBgwYpMjJSkZGRSk5O1htvvOFbr62tVXp6uqKjo9WpUyelpaWpqqrKxokBAEAgsTVkevToocWLF6u4uFh79+7VmDFjdNttt+n999+XJGVmZmrTpk1av369du7cqYqKCk2aNMnOkQEAQAAJtvOLT5gwodHjxx9/XCtWrFBRUZF69OihNWvWKD8/X2PGjJEk5eXlKSkpSUVFRRo1apQdIwMAgAASMPfInDlzRuvWrdPp06eVnJys4uJi1dfXKyUlxXdMYmKi4uPjVVhY2OR5vF6vPB5Pow0AALRPtofM/v371alTJzmdTv30pz/Vhg0b1L9/f1VWVio0NFSdO3dudLzb7VZlZWWT58vJyZHL5fJtcXFxl/g7AAAAdrE9ZK688kqVlJRo9+7deuCBBzR16lR98MEHfp8vOztbNTU1vq28vLwVpwUAAIHE1ntkJCk0NFRXXHGFJGnYsGHas2eP/vu//1t33nmn6urqVF1d3eiqTFVVlWJiYpo8n9PplNPpvNRjAwCAAGD7FZlva2hokNfr1bBhwxQSEqKCggLfWmlpqcrKypScnGzjhAAAIFDYekUmOztb48aNU3x8vE6ePKn8/Hy9+eab2rJli1wul6ZNm6asrCxFRUUpMjJSM2fOVHJyMp9YAgAAkmwOmWPHjum//uu/dPToUblcLg0aNEhbtmzR97//fUnSkiVLFBQUpLS0NHm9XqWmpmr58uV2jgwAAAKIrSGzZs2a866HhYUpNzdXubm5bTQRAAAwScDdIwMAANBchAwAADAWIQMAAIxFyAAAAGMRMgAAwFiEDAAAMBYhAwAAjEXIAAAAYxEyAADAWIQMAAAwFiEDAACMRcgAAABjETIAAMBYhAwAADAWIQMAAIxFyAAAAGMRMgAAwFiEDAAAMBYhAwAAjEXIAAAAYxEyAADAWIQMAAAwFiEDAACMRcgAAABjETIAAMBYhAwAADAWIQMAAIxFyAAAAGMRMgAAwFiEDAAAMBYhAwAAjEXIAAAAYxEyAADAWIQMAAAwFiEDAACMZWvI5OTkaMSIEYqIiFC3bt00ceJElZaWNjqmtrZW6enpio6OVqdOnZSWlqaqqiqbJgYAAIHE1pDZuXOn0tPTVVRUpK1bt6q+vl5jx47V6dOnfcdkZmZq06ZNWr9+vXbu3KmKigpNmjTJxqkBAECgCLbzi2/evLnR47Vr16pbt24qLi7W9ddfr5qaGq1Zs0b5+fkaM2aMJCkvL09JSUkqKirSqFGj7BgbAAAEiIC6R6ampkaSFBUVJUkqLi5WfX29UlJSfMckJiYqPj5ehYWF5zyH1+uVx+NptAEAgPYpYEKmoaFBGRkZGj16tK666ipJUmVlpUJDQ9W5c+dGx7rdblVWVp7zPDk5OXK5XL4tLi7uUo8OAABsEjAhk56ergMHDmjdunUXdZ7s7GzV1NT4tvLy8laaEAAABBpb75H5xowZM/Taa69p165d6tGjh29/TEyM6urqVF1d3eiqTFVVlWJiYs55LqfTKafTealHBgAAAcDWKzKWZWnGjBnasGGDtm/froSEhEbrw4YNU0hIiAoKCnz7SktLVVZWpuTk5LYeFwAABBhbr8ikp6crPz9fr7zyiiIiInz3vbhcLoWHh8vlcmnatGnKyspSVFSUIiMjNXPmTCUnJ/OJJQAAYG/IrFixQpJ04403Ntqfl5enH//4x5KkJUuWKCgoSGlpafJ6vUpNTdXy5cvbeFIAABCIbA0Zy7IueExYWJhyc3OVm5vbBhMBAACTBMynlgAAAFqKkAEAAMYiZAAAgLEIGQAAYCxCBgAAGIuQAQAAxiJkAACAsQgZAABgLEIGAAAYi5ABAADGImQAAICxCBkAAGAsQgYAABiLkAEAAMYiZAAAgLEIGQAAYCxCBgAAGIuQAQAAxiJkAACAsQgZAABgLEIGAAAYi5ABAADGImQAAICxCBkAAGAsQgYAABiLkAEAAMYiZAAAgLEIGQAAYCxCBgAAGIuQAQAAxiJkAACAsQgZAABgLEIGAAAYi5ABAADGImQAAICxCBkAAGAsW0Nm165dmjBhgmJjY+VwOLRx48ZG65Zlad68eerevbvCw8OVkpKijz/+2J5hAQBAwLE1ZE6fPq3BgwcrNzf3nOtPPfWUli1bppUrV2r37t3q2LGjUlNTVVtb28aTAgCAQBRs5xcfN26cxo0bd841y7K0dOlSPfLII7rtttskSb/97W/ldru1ceNGTZ48+ZzP83q98nq9vscej6f1BwcAAAEhYO+ROXLkiCorK5WSkuLb53K5NHLkSBUWFjb5vJycHLlcLt8WFxfXFuMCAAAbBGzIVFZWSpLcbnej/W6327d2LtnZ2aqpqfFt5eXll3ROAABgH1vfWroUnE6nnE6n3WMAAIA2ELBXZGJiYiRJVVVVjfZXVVX51gAAwHdbwIZMQkKCYmJiVFBQ4Nvn8Xi0e/duJScn2zgZAAAIFLa+tXTq1CkdOnTI9/jIkSMqKSlRVFSU4uPjlZGRoUWLFqlv375KSEjQ3LlzFRsbq4kTJ9o3NAAACBi2hszevXt10003+R5nZWVJkqZOnaq1a9fqoYce0unTp3X//ferurpa1113nTZv3qywsDC7RgYAAAHE1pC58cYbZVlWk+sOh0MLFy7UwoUL23AqAABgioC9RwYAAOBCCBkAAGAsQgYAABiLkAEAAMYiZAAAgLEIGQAAYCxCBgAAGIuQAQAAxiJkAACAsQgZAABgLEIGAAAYi5ABAADGImQAAICxCBkAAGAsQgYAABiLkAEAAMYiZAAAgLEIGQAAYCxCBgAAGIuQAQAAxgq2ewCT9Xr4dbtHOMuni8df8Bjmbj3NmRsAcOlwRQYAABiLkAEAAMYiZAAAgLEIGQAAYCxCBgAAGIuQAQAAxiJkAACAsQgZAABgLEIGAAAYi5ABAADGImQAAICxCBkAAGAsQgYAABiLkAEAAMYyImRyc3PVq1cvhYWFaeTIkXrnnXfsHgkAAASAgA+ZP/zhD8rKytL8+fO1b98+DR48WKmpqTp27JjdowEAAJsFfMg8++yzmj59uu655x71799fK1eu1GWXXabf/OY3do8GAABsFmz3AOdTV1en4uJiZWdn+/YFBQUpJSVFhYWF53yO1+uV1+v1Pa6pqZEkeTyeVp+vwfuvVj/nxWrO98ncrYe521Zz/x4H2uzM3baYu21div9f/+95Lcs6/4FWAPviiy8sSdbbb7/daP+DDz5oXXPNNed8zvz58y1JbGxsbGxsbO1gKy8vP28rBPQVGX9kZ2crKyvL97ihoUHHjx9XdHS0HA6HjZO1Px6PR3FxcSovL1dkZKTd47R7vN5ti9e7bfF6ty0TXm/LsnTy5EnFxsae97iADpnvfe976tChg6qqqhrtr6qqUkxMzDmf43Q65XQ6G+3r3LnzpRoRkiIjIwP2L0J7xOvdtni92xavd9sK9Nfb5XJd8JiAvtk3NDRUw4YNU0FBgW9fQ0ODCgoKlJycbONkAAAgEAT0FRlJysrK0tSpUzV8+HBdc801Wrp0qU6fPq177rnH7tEAAIDNAj5k7rzzTn355ZeaN2+eKisrNWTIEG3evFlut9vu0b7znE6n5s+ff9Zbebg0eL3bFq932+L1blvt6fV2WNaFPtcEAAAQmAL6HhkAAIDzIWQAAICxCBkAAGAsQgYAABiLkEGL5OTkaMSIEYqIiFC3bt00ceJElZaW2j3Wd8bixYvlcDiUkZFh9yjt1hdffKG77rpL0dHRCg8P18CBA7V37167x2qXzpw5o7lz5yohIUHh4eHq06ePHnvssQv/bh00265duzRhwgTFxsbK4XBo48aNjdYty9K8efPUvXt3hYeHKyUlRR9//LE9w/qJkEGL7Ny5U+np6SoqKtLWrVtVX1+vsWPH6vTp03aP1u7t2bNHL7zwggYNGmT3KO3WiRMnNHr0aIWEhOiNN97QBx98oGeeeUZdunSxe7R26cknn9SKFSv0/PPP68MPP9STTz6pp556Ss8995zdo7Ubp0+f1uDBg5Wbm3vO9aeeekrLli3TypUrtXv3bnXs2FGpqamqra1t40n9x8evcVG+/PJLdevWTTt37tT1119v9zjt1qlTp3T11Vdr+fLlWrRokYYMGaKlS5faPVa78/DDD+tvf/ub/vrXv9o9ynfCrbfeKrfbrTVr1vj2paWlKTw8XL///e9tnKx9cjgc2rBhgyZOnCjp31djYmNj9fOf/1y/+MUvJEk1NTVyu91au3atJk+ebOO0zccVGVyUmpoaSVJUVJTNk7Rv6enpGj9+vFJSUuwepV179dVXNXz4cN1+++3q1q2bhg4dqtWrV9s9Vrt17bXXqqCgQAcPHpQkvffee3rrrbc0btw4myf7bjhy5IgqKysb/bvicrk0cuRIFRYW2jhZywT8T/ZF4GpoaFBGRoZGjx6tq666yu5x2q1169Zp37592rNnj92jtHuHDx/WihUrlJWVpV/96lfas2ePZs2apdDQUE2dOtXu8dqdhx9+WB6PR4mJierQoYPOnDmjxx9/XFOmTLF7tO+EyspKSTrrJ+W73W7fmgkIGfgtPT1dBw4c0FtvvWX3KO1WeXm5Zs+era1btyosLMzucdq9hoYGDR8+XE888YQkaejQoTpw4IBWrlxJyFwCL730kl588UXl5+drwIABKikpUUZGhmJjY3m90Wy8tQS/zJgxQ6+99pp27NihHj162D1Ou1VcXKxjx47p6quvVnBwsIKDg7Vz504tW7ZMwcHBOnPmjN0jtivdu3dX//79G+1LSkpSWVmZTRO1bw8++KAefvhhTZ48WQMHDtTdd9+tzMxM5eTk2D3ad0JMTIwkqaqqqtH+qqoq35oJCBm0iGVZmjFjhjZs2KDt27crISHB7pHatZtvvln79+9XSUmJbxs+fLimTJmikpISdejQwe4R25XRo0ef9eMEDh48qJ49e9o0Ufv2r3/9S0FBjf8b6tChgxoaGmya6LslISFBMTExKigo8O3zeDzavXu3kpOTbZysZXhrCS2Snp6u/Px8vfLKK4qIiPC9j+pyuRQeHm7zdO1PRETEWfcfdezYUdHR0dyXdAlkZmbq2muv1RNPPKE77rhD77zzjlatWqVVq1bZPVq7NGHCBD3++OOKj4/XgAED9O677+rZZ5/Vvffea/do7capU6d06NAh3+MjR46opKREUVFRio+PV0ZGhhYtWqS+ffsqISFBc+fOVWxsrO+TTUawgBaQdM4tLy/P7tG+M2644QZr9uzZdo/Rbm3atMm66qqrLKfTaSUmJlqrVq2ye6R2y+PxWLNnz7bi4+OtsLAwq3fv3tacOXMsr9dr92jtxo4dO875b/bUqVMty7KshoYGa+7cuZbb7bacTqd18803W6WlpfYO3UL8HBkAAGAs7pEBAADGImQAAICxCBkAAGAsQgYAABiLkAEAAMYiZAAAgLEIGQAAYCxCBgAAGIuQAdCqHA6HNm7caPcYflmwYIGGDBlyUef49NNP5XA4VFJS0iozATg/QgZAs1VWVmrmzJnq3bu3nE6n4uLiNGHChEa/dM5ON954ozIyMuweA0Ab4pdGAmiWTz/9VKNHj1bnzp319NNPa+DAgaqvr9eWLVuUnp6ujz76yO4RAXwHcUUGQLP87Gc/k8Ph0DvvvKO0tDT169dPAwYMUFZWloqKipp83i9/+Uv169dPl112mXr37q25c+eqvr7et/7ee+/ppptuUkREhCIjIzVs2DDt3btXkvTZZ59pwoQJ6tKlizp27KgBAwboz3/+s9/fw4Vm+cYLL7yguLg4XXbZZbrjjjtUU1PTaP1//ud/lJSUpLCwMCUmJmr58uV+zwTg4nBFBsAFHT9+XJs3b9bjjz+ujh07nrXeuXPnJp8bERGhtWvXKjY2Vvv379f06dMVERGhhx56SJI0ZcoUDR06VCtWrFCHDh1UUlKikJAQSVJ6errq6uq0a9cudezYUR988IE6derk9/dxoVkk6dChQ3rppZe0adMmeTweTZs2TT/72c/04osvSpJefPFFzZs3T88//7yGDh2qd999V9OnT1fHjh01depUv2cD4Ce7f/02gMC3e/duS5L18ssvX/BYSdaGDRuaXH/66aetYcOG+R5HRERYa9euPeexAwcOtBYsWNDsOW+44QZr9uzZzT7+27PMnz/f6tChg/X555/79r3xxhtWUFCQdfToUcuyLKtPnz5Wfn5+o/M89thjVnJysmVZlnXkyBFLkvXuu+82ew4A/uOKDIALsizL7+f+4Q9/0LJly/TJJ5/o1KlT+vrrrxUZGelbz8rK0n333aff/e53SklJ0e23364+ffpIkmbNmqUHHnhAf/nLX5SSkqK0tDQNGjToks0iSfHx8br88st9j5OTk9XQ0KDS0lJFRETok08+0bRp0zR9+nTfMV9//bVcLpffcwHwH/fIALigvn37yuFwtPiG3sLCQk2ZMkW33HKLXnvtNb377ruaM2eO6urqfMcsWLBA77//vsaPH6/t27erf//+2rBhgyTpvvvu0+HDh3X33Xdr//79Gj58uJ577jm/vofmzHIhp06dkiStXr1aJSUlvu3AgQPnvU8IwKVDyAC4oKioKKWmpio3N1enT58+a726uvqcz3v77bfVs2dPzZkzR8OHD1ffvn312WefnXVcv379lJmZqb/85S+aNGmS8vLyfGtxcXH66U9/qpdfflk///nPtXr1ar++h+bOUlZWpoqKCt/joqIiBQUF6corr5Tb7VZsbKwOHz6sK664otGWkJDg11wALg5vLQFoltzcXI0ePVrXXHONFi5cqEGDBunrr7/W1q1btWLFCn344YdnPadv374qKyvTunXrNGLECL3++uu+qy2S9NVXX+nBBx/UD3/4QyUkJOjzzz/Xnj17lJaWJknKyMjQuHHj1K9fP504cUI7duxQUlLSeef88ssvz/phdN27d7/gLN8ICwvT1KlT9etf/1oej0ezZs3SHXfcoZiYGEnSo48+qlmzZsnlcukHP/iBvF6v9u7dqxMnTigrK6ulLyuAi2X3TToAzFFRUWGlp6dbPXv2tEJDQ63LL7/c+s///E9rx44dvmP0rZt9H3zwQSs6Otrq1KmTdeedd1pLliyxXC6XZVmW5fV6rcmTJ1txcXFWaGioFRsba82YMcP66quvLMuyrBkzZlh9+vSxnE6n1bVrV+vuu++2/vGPfzQ53w033GBJOmt77LHHLjiLZf37Zt/Bgwdby5cvt2JjY62wsDDrhz/8oXX8+PFGX+fFF1+0hgwZYoWGhlpdunSxrr/+et+N0NzsC7Qth2VdxF18AAAANuIeGQAAYCxCBgAAGIuQAQAAxiJkAACAsQgZAABgLEIGAAAYi5ABAADGImQAAICxCBkAAGAsQgYAABiLkAEAAMb6fwiweN8dw6enAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nums_samples = len(labels)\n",
        "class_weight = {cls: nums_samples/count for cls, count in counts.items()}\n",
        "sample_weight = [class_weight[label] for label in labels]\n",
        "print(\"Sample weight:\\n\", sample_weight)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AjMdlEfEaRL7",
        "outputId": "c74a3362-0c72-4cca-b283-8ab2b591e183"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample weight:\n",
            " [100.0, 100.0, 100.0, 10.0, 10.0, 10.0, 10.0, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 100.0, 100.0, 100.0, 10.0, 10.0, 10.0, 10.0, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 100.0, 100.0, 100.0, 10.0, 10.0, 10.0, 10.0, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 100.0, 100.0, 100.0, 10.0, 10.0, 10.0, 10.0, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 10.0, 10.0, 10.0, 10.0, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 10.0, 10.0, 10.0, 10.0, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 10.0, 10.0, 10.0, 10.0, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 10.0, 10.0, 10.0, 10.0, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 10.0, 10.0, 10.0, 10.0, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 10.0, 10.0, 10.0, 10.0, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 10.0, 10.0, 10.0, 10.0, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 10.0, 10.0, 10.0, 10.0, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 10.0, 10.0, 10.0, 10.0, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 10.0, 10.0, 10.0, 10.0, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 10.0, 10.0, 10.0, 10.0, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 10.0, 10.0, 10.0, 10.0, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 10.0, 10.0, 10.0, 10.0, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 10.0, 10.0, 10.0, 10.0, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 10.0, 10.0, 10.0, 10.0, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 10.0, 10.0, 10.0, 10.0, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 10.0, 10.0, 10.0, 10.0, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 10.0, 10.0, 10.0, 10.0, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 10.0, 10.0, 10.0, 10.0, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 10.0, 10.0, 10.0, 10.0, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 10.0, 10.0, 10.0, 10.0, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 10.0, 10.0, 10.0, 10.0, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 10.0, 10.0, 10.0, 10.0, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 10.0, 10.0, 10.0, 10.0, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 10.0, 10.0, 10.0, 10.0, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 10.0, 10.0, 10.0, 10.0, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 10.0, 10.0, 10.0, 10.0, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 10.0, 10.0, 10.0, 10.0, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 10.0, 10.0, 10.0, 10.0, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 10.0, 10.0, 10.0, 10.0, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 10.0, 10.0, 10.0, 10.0, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 10.0, 10.0, 10.0, 10.0, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 10.0, 10.0, 10.0, 10.0, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 10.0, 10.0, 10.0, 10.0, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 10.0, 10.0, 10.0, 10.0, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425, 10.0, 10.0, 10.0, 10.0, 5.2631578947368425, 5.2631578947368425, 5.2631578947368425]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import WeightedRandomSampler\n",
        "sampler = WeightedRandomSampler(weights = sample_weight, num_samples=len(sample_weight), replacement = True)\n",
        "train_loader = torch.utils.data.DataLoader(datapoints, batch_size = 32, sampler = sampler)\n"
      ],
      "metadata": {
        "id": "Wq9LEI3UajgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(dataset_dir, transforms=train_transforms)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tlp3KIaFQIZO",
        "outputId": "e31f6cdc-5213-47d3-e7b6-48574cd7cd47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0 loss: 5.6077, val acc: 0.14\n",
            "epoch 1 loss: 3.1112, val acc: 0.48\n",
            "epoch 2 loss: 2.2976, val acc: 0.3\n",
            "epoch 3 loss: 1.9245, val acc: 0.36\n",
            "epoch 4 loss: 2.0061, val acc: 0.41\n",
            "epoch 5 loss: 1.2280, val acc: 0.42\n",
            "epoch 6 loss: 1.0257, val acc: 0.52\n",
            "epoch 7 loss: 0.8482, val acc: 0.31\n",
            "epoch 8 loss: 0.7571, val acc: 0.2\n",
            "epoch 9 loss: 1.3117, val acc: 0.35\n",
            "epoch 10 loss: 0.4040, val acc: 0.3\n",
            "epoch 11 loss: 0.7601, val acc: 0.42\n",
            "epoch 12 loss: 0.4983, val acc: 0.44\n",
            "epoch 13 loss: 0.3232, val acc: 0.41\n",
            "epoch 14 loss: 0.2570, val acc: 0.19\n",
            "epoch 15 loss: 0.1648, val acc: 0.39\n",
            "epoch 16 loss: 0.3815, val acc: 0.3\n",
            "epoch 17 loss: 0.2173, val acc: 0.37\n",
            "epoch 18 loss: 0.1537, val acc: 0.3\n",
            "epoch 19 loss: 0.0524, val acc: 0.48\n",
            "epoch 20 loss: 0.3950, val acc: 0.14\n",
            "epoch 21 loss: 0.2314, val acc: 0.41\n",
            "epoch 22 loss: 0.2792, val acc: 0.44\n",
            "epoch 23 loss: 0.0354, val acc: 0.48\n",
            "epoch 24 loss: 0.0897, val acc: 0.47\n",
            "epoch 25 loss: 0.0746, val acc: 0.48\n",
            "epoch 26 loss: 0.0245, val acc: 0.49\n",
            "epoch 27 loss: 0.0493, val acc: 0.47\n",
            "epoch 28 loss: 0.0080, val acc: 0.47\n",
            "epoch 29 loss: 0.2073, val acc: 0.38\n",
            "epoch 30 loss: 0.2390, val acc: 0.47\n",
            "epoch 31 loss: 0.0250, val acc: 0.49\n",
            "epoch 32 loss: 0.1801, val acc: 0.45\n",
            "epoch 33 loss: 0.1342, val acc: 0.36\n",
            "epoch 34 loss: 0.0912, val acc: 0.51\n",
            "epoch 35 loss: 0.2040, val acc: 0.23\n",
            "epoch 36 loss: 0.0498, val acc: 0.4\n",
            "epoch 37 loss: 0.0531, val acc: 0.41\n",
            "epoch 38 loss: 0.0301, val acc: 0.44\n",
            "epoch 39 loss: 0.0192, val acc: 0.29\n",
            "epoch 40 loss: 0.0183, val acc: 0.53\n",
            "epoch 41 loss: 0.0435, val acc: 0.47\n",
            "epoch 42 loss: 0.2844, val acc: 0.14\n",
            "epoch 43 loss: 0.5061, val acc: 0.23\n",
            "epoch 44 loss: 0.3595, val acc: 0.5\n",
            "epoch 45 loss: 0.2768, val acc: 0.45\n",
            "epoch 46 loss: 0.1037, val acc: 0.48\n",
            "epoch 47 loss: 0.1048, val acc: 0.49\n",
            "epoch 48 loss: 0.0356, val acc: 0.52\n",
            "epoch 49 loss: 0.1556, val acc: 0.31\n",
            "epoch 50 loss: 0.1387, val acc: 0.44\n",
            "epoch 51 loss: 0.0436, val acc: 0.48\n",
            "epoch 52 loss: 0.0479, val acc: 0.48\n",
            "epoch 53 loss: 0.0412, val acc: 0.5\n",
            "epoch 54 loss: 0.0401, val acc: 0.47\n",
            "epoch 55 loss: 0.0028, val acc: 0.51\n",
            "epoch 56 loss: 0.0616, val acc: 0.3\n",
            "epoch 57 loss: 0.1239, val acc: 0.34\n",
            "epoch 58 loss: 0.3623, val acc: 0.42\n",
            "epoch 59 loss: 0.1857, val acc: 0.4\n",
            "epoch 60 loss: 0.0323, val acc: 0.42\n",
            "epoch 61 loss: 0.0247, val acc: 0.46\n",
            "epoch 62 loss: 0.0162, val acc: 0.44\n",
            "epoch 63 loss: 0.0367, val acc: 0.39\n",
            "epoch 64 loss: 0.1036, val acc: 0.49\n",
            "epoch 65 loss: 0.0341, val acc: 0.48\n",
            "epoch 66 loss: 0.0568, val acc: 0.33\n",
            "epoch 67 loss: 0.0164, val acc: 0.47\n",
            "epoch 68 loss: 0.3397, val acc: 0.42\n",
            "epoch 69 loss: 0.1705, val acc: 0.44\n",
            "epoch 70 loss: 0.1152, val acc: 0.43\n",
            "epoch 71 loss: 0.1593, val acc: 0.4\n",
            "epoch 72 loss: 0.0645, val acc: 0.42\n",
            "epoch 73 loss: 0.0060, val acc: 0.45\n",
            "epoch 74 loss: 0.0118, val acc: 0.43\n",
            "epoch 75 loss: 0.0076, val acc: 0.42\n",
            "epoch 76 loss: 0.0023, val acc: 0.44\n",
            "epoch 77 loss: 0.0334, val acc: 0.44\n",
            "epoch 78 loss: 0.1492, val acc: 0.41\n",
            "epoch 79 loss: 0.0720, val acc: 0.42\n",
            "epoch 80 loss: 0.0535, val acc: 0.4\n",
            "epoch 81 loss: 0.1171, val acc: 0.44\n",
            "epoch 82 loss: 0.0430, val acc: 0.43\n",
            "epoch 83 loss: 0.0428, val acc: 0.23\n",
            "epoch 84 loss: 0.0130, val acc: 0.45\n",
            "epoch 85 loss: 0.0252, val acc: 0.43\n",
            "epoch 86 loss: 0.0413, val acc: 0.43\n",
            "epoch 87 loss: 0.0007, val acc: 0.43\n",
            "epoch 88 loss: 0.0199, val acc: 0.42\n",
            "epoch 89 loss: 0.0187, val acc: 0.28\n",
            "epoch 90 loss: 0.0174, val acc: 0.44\n",
            "epoch 91 loss: 0.0003, val acc: 0.45\n",
            "epoch 92 loss: 0.0003, val acc: 0.45\n",
            "epoch 93 loss: 0.0009, val acc: 0.45\n",
            "epoch 94 loss: 0.0001, val acc: 0.45\n",
            "epoch 95 loss: 0.0009, val acc: 0.44\n",
            "epoch 96 loss: 0.0004, val acc: 0.44\n",
            "epoch 97 loss: 0.0001, val acc: 0.45\n",
            "epoch 98 loss: 0.0017, val acc: 0.45\n",
            "epoch 99 loss: 0.0001, val acc: 0.45\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that our validation accuracy is not great, even though training loss is going down. Let's see if we can improve our model performance during this session.\n",
        "\n",
        "Feel free to briefly jot down your thoughts and ideas, prioritize them and then start adjusting the code in this notebook."
      ],
      "metadata": {
        "id": "rQ4gufJtqlj1"
      }
    }
  ]
}